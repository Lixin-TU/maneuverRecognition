{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real world vehicle maneuver recognition using smartphone sensors and LSTM models"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import packages:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T10:51:19.497206Z",
     "start_time": "2023-10-02T10:51:15.024721Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mplotly\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexpress\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpx\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mplotly\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgraph_objs\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mgo\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msignal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m savgol_filter\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LabelEncoder\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RobustScaler\n",
      "File \u001B[0;32m/opt/anaconda3/envs/p3-8-pytorch01/lib/python3.8/site-packages/scipy/signal/__init__.py:331\u001B[0m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_spectral_py\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_wavelets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m--> 331\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_peak_finding\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_czt\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwindows\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_window  \u001B[38;5;66;03m# keep this one in signal namespace\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/p3-8-pytorch01/lib/python3.8/site-packages/scipy/signal/_peak_finding.py:8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msignal\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_wavelets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cwt, ricker\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstats\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m scoreatpercentile\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_peak_finding_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     11\u001B[0m     _local_maxima_1d,\n\u001B[1;32m     12\u001B[0m     _select_by_peak_distance,\n\u001B[1;32m     13\u001B[0m     _peak_prominences,\n\u001B[1;32m     14\u001B[0m     _peak_widths\n\u001B[1;32m     15\u001B[0m )\n\u001B[1;32m     18\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124margrelmin\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124margrelmax\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124margrelextrema\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpeak_prominences\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     19\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpeak_widths\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfind_peaks\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfind_peaks_cwt\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/p3-8-pytorch01/lib/python3.8/site-packages/scipy/stats/__init__.py:485\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m.. _statsrefmanual:\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    480\u001B[0m \n\u001B[1;32m    481\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_warnings_errors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001B[1;32m    484\u001B[0m                                DegenerateDataWarning, FitError)\n\u001B[0;32m--> 485\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_stats_py\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m    486\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_variation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m variation\n\u001B[1;32m    487\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistributions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/p3-8-pytorch01/lib/python3.8/site-packages/scipy/stats/_stats_py.py:46\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspecial\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mspecial\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m linalg\n\u001B[0;32m---> 46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distributions\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _mstats_basic \u001B[38;5;28;01mas\u001B[39;00m mstats_basic\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_stats_mstats_common\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (_find_repeats, linregress, theilslopes,\n\u001B[1;32m     49\u001B[0m                                    siegelslopes)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/p3-8-pytorch01/lib/python3.8/site-packages/scipy/stats/distributions.py:10\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Author:  Travis Oliphant  2002-2011 with contributions from\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#          SciPy Developers 2004-2011\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#       instead of `git blame -Lxxx,+x`.\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_distn_infrastructure\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (rv_discrete, rv_continuous, rv_frozen)\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _continuous_distns\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _discrete_distns\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_continuous_distns\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/p3-8-pytorch01/lib/python3.8/site-packages/scipy/stats/_continuous_distns.py:8540\u001B[0m\n\u001B[1;32m   8536\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_entropy\u001B[39m(\u001B[38;5;28mself\u001B[39m, c):\n\u001B[1;32m   8537\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0.5\u001B[39m\u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m-> 8540\u001B[0m triang \u001B[38;5;241m=\u001B[39m \u001B[43mtriang_gen\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtriang\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   8543\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mtruncexpon_gen\u001B[39;00m(rv_continuous):\n\u001B[1;32m   8544\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"A truncated exponential continuous random variable.\u001B[39;00m\n\u001B[1;32m   8545\u001B[0m \n\u001B[1;32m   8546\u001B[0m \u001B[38;5;124;03m    %(before_notes)s\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   8563\u001B[0m \n\u001B[1;32m   8564\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/p3-8-pytorch01/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py:1936\u001B[0m, in \u001B[0;36mrv_continuous.__init__\u001B[0;34m(self, momtype, a, b, xtol, badvalue, name, longname, shapes, extradoc, seed)\u001B[0m\n\u001B[1;32m   1931\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mextradoc \u001B[38;5;241m=\u001B[39m extradoc\n\u001B[1;32m   1933\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_construct_argparser(meths_to_inspect\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cdf],\n\u001B[1;32m   1934\u001B[0m                           locscale_in\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloc=0, scale=1\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   1935\u001B[0m                           locscale_out\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloc, scale\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m-> 1936\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_attach_methods\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1938\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m longname \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1939\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maeiouAEIOU\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/p3-8-pytorch01/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py:1971\u001B[0m, in \u001B[0;36mrv_continuous._attach_methods\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1967\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1968\u001B[0m \u001B[38;5;124;03mAttaches dynamically created methods to the rv_continuous instance.\u001B[39;00m\n\u001B[1;32m   1969\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1970\u001B[0m \u001B[38;5;66;03m# _attach_methods is responsible for calling _attach_argparser_methods\u001B[39;00m\n\u001B[0;32m-> 1971\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_attach_argparser_methods\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1973\u001B[0m \u001B[38;5;66;03m# nin correction\u001B[39;00m\n\u001B[1;32m   1974\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ppfvec \u001B[38;5;241m=\u001B[39m vectorize(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ppf_single, otypes\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/p3-8-pytorch01/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py:669\u001B[0m, in \u001B[0;36mrv_generic._attach_argparser_methods\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    661\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    662\u001B[0m \u001B[38;5;124;03mGenerates the argument-parsing functions dynamically and attaches\u001B[39;00m\n\u001B[1;32m    663\u001B[0m \u001B[38;5;124;03mthem to the instance.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    666\u001B[0m \u001B[38;5;124;03mduring unpickling (__setstate__)\u001B[39;00m\n\u001B[1;32m    667\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    668\u001B[0m ns \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 669\u001B[0m \u001B[43mexec\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parse_arg_template\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mns\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[38;5;66;03m# NB: attach to the instance, not class\u001B[39;00m\n\u001B[1;32m    671\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_parse_args\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_parse_args_stats\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_parse_args_rvs\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n",
      "File \u001B[0;32m<string>:2\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from numpy.random import default_rng\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T10:51:19.508683Z",
     "start_time": "2023-10-02T10:51:19.502459Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1896)\n",
    "random.seed(1896)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "The dataset consists of 19 variables and the recording of three different people with different vehicles. Each driving maneuver was assigned a maneuver type, the route section, and road type as well as an unique maneuver ID. The smartphone sensor data includes datetime, acceleration on and rotation around three orthogonal axes, as well as GPS information such as longitude, latitude, altitude, accuracy and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.504347Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/SensorRec_data_eng.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning & preprocessing:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "Real world data of smartphone sensors used to record driving behaviour will always be noisy due to the vehicle's own vibrations as well as road surface conditions and irregularities. The data from the smartphone sensors is noisy due to the vehicle's own vibrations and the bumps in the road. To reduce this noise, filtering can be applied. One method to smooth noisy data is the Savitzky-Golay filter. It applies a polynomial regression to windows of an arbitrary width.\n",
    "\n",
    "\n",
    "One advantage of the Savitzky-Golay filter over methods such as a moving average is that high frequencies are not simply cut off, but are included in the calculation. This allows the filter to preserve important properties of the distribution such as relative maxima or minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.507479Z"
    }
   },
   "outputs": [],
   "source": [
    "snippet = df.iloc[400:900,:].copy()\n",
    "snippet['filtered_accX'] = savgol_filter(snippet['accX'], 20, 6)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=snippet.datetime, \n",
    "                         y=snippet.accX, mode='lines'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=snippet.datetime, \n",
    "                         y=snippet.filtered_accX, mode='lines'))\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply this filter to all variables of the accelerometer and gyroscope. In a more detailed investigation it would be possible to study the effect of using different polynomial orders for different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.509691Z"
    }
   },
   "outputs": [],
   "source": [
    "vars_to_filter = ['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ']\n",
    "\n",
    "for var in vars_to_filter:\n",
    "    df[var] = savgol_filter(df[var], window_length=20, polyorder=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the amount of recorded maneuver types\n",
    "\n",
    "To examine how many individual driving maneuvers were recorded per maneuver type, the following plot can be considered. It turns out that some maneuvers, such as overtaking or crossing an intersection, occurred in too few cases overall. Therefore these maneuvers should be excluded. However, since the direct removal of corresponding time points would lead to breaks in the time series and thus additionally unnatural transitions from the remaining maneuvers to each other would arise, this is done only at a later point, after the execution of windowing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.511992Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_maneuvers = df.drop_duplicates(subset=['maneuverID','maneuverType'])['maneuverType'].value_counts()\n",
    "\n",
    "fig = px.bar(x=unique_maneuvers.index, y=unique_maneuvers.values,\n",
    "            title='Amount of maneuvers per type')\n",
    "\n",
    "fig.update_xaxes(title_text='maneuver')\n",
    "fig.update_yaxes(title_text='Amount')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing and Train-/Test-Splitting\n",
    "\n",
    "Our goal is to create a model which can correctly identify driving maneuvers in unknown data of future driving recordings. Therefore, to train the model, we cannot simply use the individual maneuvers already assigned to their IDs, but have to use a sliding window to create training and testing cases.\n",
    "\n",
    "In order to do that, we have to create two specific functions:\n",
    "1. A windowing function that gets our data and returns windows with a specific amount of time steps and a speficifc step length between those windows in the shape of (cases, timesteps, variables).\n",
    "2. A function to split the data into training and testing data before the windowing is applied. If we do not split training and testing data before using the windowing function, there will be data leakage because overlapping windows may end up in both partitions.\n",
    "\n",
    "The 'create_dataset' function for the windowing is very simple since it only extracts the values for each time step and takes the mode of the label (our maneuver type) as the main label for the window.\n",
    "\n",
    "For splitting the data into training and testing parts we create the 'sliced_train_test' function which can split the dataframe in a arbitrary number of partitions. Before we randomly select some of these in regard to our test size and apply the windowing function to them, we drop these partitions from our dataframe and create a scaler based on the remaining training data. \n",
    "\n",
    "With this approach, it can still happen occasionally that we create unnatural transitions between maneuvers that occurred before and after the part we cut out or that we break some maneuvers. However, the amount of it which depends on the number of splits we use will be negligible, as it is clearly more important that we have absolutely no data leakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.514976Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(X, y, time_steps=1, step=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(0, len(X) - time_steps, step):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        labels = y.iloc[i: i + time_steps]\n",
    "        Xs.append(v)\n",
    "        ys.append(labels.mode()[0])\n",
    "        #ys.append(stats.mode(labels)[0][0]) # removed because of DeprecationWarning\n",
    "    return np.array(Xs), np.array(ys).reshape(-1, 1)\n",
    "\n",
    "def sliced_train_test(df, X_vars, y_var, splits, test_size, time_steps, stepsize):\n",
    "    split_list = np.array_split(df, splits)\n",
    "    \n",
    "    rng = default_rng()\n",
    "    numbers = rng.choice(splits, size=math.floor(splits*test_size), replace=False)\n",
    "\n",
    "    df_full = df.copy()\n",
    "\n",
    "    # remove test slices from dataframe so that we can create training data \n",
    "    # from everything thats left\n",
    "    for num in numbers:\n",
    "        df = df.drop(split_list[num].index)\n",
    "\n",
    "    # scale based on training data\n",
    "    scaler = RobustScaler()\n",
    "    scaler = scaler.fit(df[X_vars].values)\n",
    "    df.loc[:, X_vars] = scaler.transform(df[X_vars].to_numpy())\n",
    "\n",
    "    # sample test splits based on random numbers\n",
    "    X_test, y_test = [], []\n",
    "    for num in numbers:\n",
    "        split_idx = split_list[num].index\n",
    "        \n",
    "        extract = df_full.loc[split_idx,:]\n",
    "        extract.loc[:, X_vars] = scaler.transform(extract[X_vars].to_numpy())\n",
    "        \n",
    "        X_slice, y_slice = create_dataset(extract[X_vars], extract[y_var], time_steps, stepsize)\n",
    "\n",
    "        X_test.append(X_slice)\n",
    "        y_test.append(y_slice)\n",
    "    \n",
    "    X_test, y_test = np.vstack(X_test), np.vstack(y_test)\n",
    "\n",
    "    X_train, y_train = create_dataset(df[X_vars], df[y_var], time_steps, stepsize)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply our functions. In order to avoid mixing the trips of the three test subjects within the windowing function and to ensure that different route segments of the subjects can appear in the training and test data sets, we apply the function individually. Our windows will have a length of 20 timesteps which is 10 seconds in our dataset and we will use a stepsize of 4 timesteps which equals 2 seconds. We then merge the data, with every window seperated and already in the right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.518133Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p1 = df.loc[df['person']=='P01',:]\n",
    "df_p2 = df.loc[df['person']=='P02',:]\n",
    "df_p3 = df.loc[df['person']=='P03',:]\n",
    "\n",
    "x_vars = ['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ', 'speed']\n",
    "y_var = 'maneuverType'\n",
    "splits = 40\n",
    "test_size = 0.2\n",
    "time_steps = 20\n",
    "stepsize = 4\n",
    "\n",
    "X_train1, y_train1, X_test1, y_test1 = sliced_train_test(df_p1, x_vars, y_var, splits, test_size,\n",
    "                                                         time_steps, stepsize)\n",
    "\n",
    "X_train2, y_train2, X_test2, y_test2 = sliced_train_test(df_p2, x_vars, y_var, splits, test_size,\n",
    "                                                         time_steps, stepsize)\n",
    "\n",
    "X_train3, y_train3, X_test3, y_test3 = sliced_train_test(df_p3, x_vars, y_var, splits, test_size,\n",
    "                                                         time_steps, stepsize)\n",
    "\n",
    "X_train = np.vstack((X_train1, X_train2, X_train3))\n",
    "y_train = np.vstack((y_train1, y_train2, y_train3))\n",
    "X_test = np.vstack((X_test1, X_test2, X_test3))\n",
    "y_test = np.vstack((y_test1, y_test2, y_test3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing rare maneuvers and undersampling the class with way too much samples\n",
    "\n",
    "At the beginning we could already see that certain maneuvers occurred in a way too small amount of cases and that there is also an unbalanced distribution. Because of what we just did with the windowing, we can now drop those maneuvers without affecting others. But first lets check the distribution again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.520496Z"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will remove those rare maneuvers. In the case of the maneuvers 'stationary' and 'continuous_driving', which are present in too high a number of cases, we will apply undersampling and remove individual windows. For 'continuous_driving' we will can set the proportion of cases to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.523564Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_maneuvers = ['acceleration_lane', 'overtaking', 'deceleration_lane', 'crossing_roundabout', 'crossing_intersection']\n",
    "maneuvers_to_drop_train = np.where(np.isin(y_train, drop_maneuvers))[0]\n",
    "maneuvers_to_drop_test = np.where(np.isin(y_test, drop_maneuvers))[0]\n",
    "\n",
    "stationary_to_drop_train = np.where(y_train == 'stationary')[0][::2] # every 2nd\n",
    "stationary_to_drop_test = np.where(y_test == 'stationary')[0][::2] # every 2nd\n",
    "\n",
    "cd_maneuvers_train = np.where(y_train == 'continuous_driving')[0]\n",
    "cd_maneuvers_test = np.where(y_test == 'continuous_driving')[0]\n",
    "\n",
    "proportion_to_drop = 0.9\n",
    "cd_drop_train = random.sample(list(cd_maneuvers_train),\n",
    "                               k=round(proportion_to_drop*len(cd_maneuvers_train)))\n",
    "cd_drop_test = random.sample(list(cd_maneuvers_test),\n",
    "                               k=round(proportion_to_drop*len(cd_maneuvers_test)))\n",
    "\n",
    "to_drop_train = np.hstack((maneuvers_to_drop_train, cd_drop_train, stationary_to_drop_train))\n",
    "to_drop_test = np.hstack((maneuvers_to_drop_test, cd_drop_test, stationary_to_drop_test))\n",
    "\n",
    "\n",
    "#len(cd_maneuvers_train), amount_to_drop_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T10:51:19.551547Z",
     "start_time": "2023-10-02T10:51:19.525922Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape, len(to_drop_train), len(to_drop_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.528790Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.delete(X_train, to_drop_train, axis=0)\n",
    "y_train = np.delete(y_train, to_drop_train)\n",
    "\n",
    "X_test = np.delete(X_test, to_drop_test, axis=0)\n",
    "y_test = np.delete(y_test, to_drop_test)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have another look at the amount of windows in our new training and testing partitions. There will still be slight imbalance between our maneuver classes, but now the data set is much better suited for training and testing our model and since we are dealing with real world data it is fine not to synthetically bring the data to an absolute balanced ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.531156Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Test data', x=np.unique(y_train), y=np.unique(y_train, return_counts=True)[1]),\n",
    "    go.Bar(name='Training data', x=np.unique(y_test), y=np.unique(y_test, return_counts=True)[1])\n",
    "])\n",
    "\n",
    "fig.update_layout(barmode='stack', title=\"Amount of windows per maneuver type in the train and test data\")\n",
    "fig.update_xaxes(title_text='maneuver')\n",
    "fig.update_yaxes(title_text='Amount')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding and final transformation\n",
    "\n",
    "Now we have to create a label encoder so that we can extract the correct labels of the predictions later. The last step is to turn our training and testing data into the format of a PyTorch variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.534196Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(y_train)\n",
    "\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "X_train = Variable(torch.from_numpy(X_train)).float()\n",
    "y_train = Variable(torch.from_numpy(y_train)).long()\n",
    "X_test  = Variable(torch.from_numpy(X_test)).float()\n",
    "y_test  = Variable(torch.from_numpy(y_test)).long()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model architecture\n",
    "\n",
    "Our next step is to define the architecture of the model. In the process, different network structures can be tested. The following structure we use here has proven to be promising:\n",
    "The network is first composed of a multi-layer long short-term memory (LSTM) RNN with four stacked layers (n_layers) and 24 outputs to the fully-connected layer (n_hidden). Additionally we implement a dropout of 0.7, which is a very high value, but showed a positive effect on the generalizability of the model. The fully-connected layer consists of two relu layers with a dropout of 0.3 and a final linear layer that produces the output with the dimension of our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.536402Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class maneuverModel(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, n_hidden=24, n_layers=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = n_features,\n",
    "            hidden_size = n_hidden,\n",
    "            num_layers = n_layers,\n",
    "            batch_first = True, # Format\n",
    "            dropout = 0.7\n",
    "        )\n",
    "\n",
    "        self.full_layer1 = nn.Linear(n_hidden, 64) # vorher 128\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.full_layer2 = nn.Linear(64, 32)\n",
    "\n",
    "        self.classifier = nn.Linear(32, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "\n",
    "        out = hidden[-1]\n",
    "        #out = F.relu(self.full_layer1(out))\n",
    "        out = self.dropout(F.relu(self.full_layer1(out)))\n",
    "        out = F.relu(self.full_layer2(out))\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we also need are functions for the training and testing processes. For this purpose we use PyTorch's wrapper DataLoader, which allows easier access to samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.539325Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            #print(f\"Train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return [correct, test_loss]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model\n",
    "\n",
    "At this point we can initialize the model. For this we set the number of features and classes and choose Adaptive Moment Estimation with a learning rate of 0.001 as optimization method. We also set CrossEntropyLoss as our loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.541213Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = X_train.shape[2]\n",
    "n_classes = len(np.unique(y_train))\n",
    "model     = maneuverModel(n_features, n_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "After we initialize our dataloader, we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.543106Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(list(zip(X_train, y_train)), batch_size=batch_size)\n",
    "test_dataloader = DataLoader(list(zip(X_test, y_test)), batch_size=batch_size)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "loss_list     = np.zeros((epochs,))\n",
    "accuracy_list = np.zeros((epochs,))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    accuracy, test_loss = test(test_dataloader, model, loss_fn)\n",
    "    accuracy_list[epoch] = accuracy\n",
    "    loss_list[epoch] = test_loss\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining the evolution of validation accuracy and validation loss, we can see how good our training process works and whether we tend to some kind of over- or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.544835Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "ax1.plot(accuracy_list)\n",
    "ax1.set_ylabel(\"validation accuracy\")\n",
    "ax2.plot(loss_list)\n",
    "ax2.set_ylabel(\"validation loss\")\n",
    "ax2.set_xlabel(\"epochs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.546735Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "ax1.plot(accuracy_list[50:])\n",
    "ax1.set_ylabel(\"validation accuracy\")\n",
    "ax2.plot(loss_list[50:])\n",
    "ax2.set_ylabel(\"validation loss\")\n",
    "ax2.set_xlabel(\"epochs - 50\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "In order to evaluate the performance of our model, the predictions of the model for our test data can be compared with the actual values in the form of a confusion matrix. Since we have an unbalanced multi-class problem, and the color intensity takes all fields into account, the distribution should be inspected row by row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.548654Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = [pred.argmax() for pred in model(X_test)]\n",
    "\n",
    "cm = confusion_matrix(label_encoder.inverse_transform(y_test), label_encoder.inverse_transform(y_pred))\n",
    "heatmap = go.Heatmap(z=cm, x=label_encoder.classes_, y=label_encoder.classes_, colorscale='Blues') # FEHLER IRGENDWO\n",
    "\n",
    "# create the layout\n",
    "layout = go.Layout(title='Confusion Matrix')\n",
    "\n",
    "# create the figure\n",
    "fig = go.Figure(data=[heatmap], layout=layout)\n",
    "\n",
    "fig.update_layout(yaxis = dict(categoryorder = 'category descending'))\n",
    "fig.update_yaxes(title_text=\"Actual\")\n",
    "fig.update_xaxes(title_text=\"Predicted\")\n",
    "\n",
    "fig.layout.height = 900\n",
    "fig.layout.width = 900\n",
    "\n",
    "# show the figure\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better view of the precision of the model, we can slightly modify the Confusion Matrix and calculate row-wise relative values for the respective classes. In this way we can better understand for each actual class what proportion of which classes were predicted. With this the diagonal elements represent a measurement in the form of a class-specific precision metric.\n",
    "\n",
    "Looking at these results, it is not only evident that certain maneuvers are already very well recognized. We also see that the misclassification of maneuvers with poorer precision is distributed among maneuvers that can logically overlap very strongly with these in practical execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-02T10:51:19.550416Z"
    }
   },
   "outputs": [],
   "source": [
    "def relativeCorrect(array):\n",
    "    return array/sum(array)\n",
    "\n",
    "def confusion_matrix_relative(y_test, y_pred):\n",
    "\n",
    "    cm = confusion_matrix(label_encoder.inverse_transform(y_test), label_encoder.inverse_transform(y_pred))\n",
    "\n",
    "    cm_relative = np.apply_along_axis(relativeCorrect, 1, cm)\n",
    "    heatmap = go.Heatmap(z=cm_relative, x=label_encoder.classes_, y=label_encoder.classes_,\n",
    "                         colorscale='Blues')\n",
    "\n",
    "\n",
    "    # create the layout\n",
    "    layout = go.Layout(title='Confusion Matrix with rowwise relative values (relative predictions per actual class)')\n",
    "\n",
    "    # create the figure\n",
    "    fig = go.Figure(data=[heatmap], layout=layout)\n",
    "\n",
    "    fig.update_layout(yaxis = dict(categoryorder = 'category descending'))\n",
    "    fig.update_yaxes(title_text=\"Actual\")\n",
    "    fig.update_xaxes(title_text=\"Predicted\")\n",
    "\n",
    "    fig.layout.height = 900\n",
    "    fig.layout.width = 900\n",
    "    # show the figure\n",
    "    fig.show()\n",
    "\n",
    "confusion_matrix_relative(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T10:51:19.554076Z",
     "start_time": "2023-10-02T10:51:19.552468Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_04_10sec_filtered_2.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "\n",
    "model = maneuverModel(n_features, n_classes)\n",
    "model.load_state_dict(torch.load(\"model_04_10sec_filtered.pth\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result can be used as a reference point for different approaches to improve maneuver recognition:\n",
    "\n",
    "- A more in-depth optimization of the model could be executed to improve the overall performance.\n",
    "- It could prove useful to develop a system that not only takes the most probable class as the prediction, but also considers the values of the second or third most probable class. This could be a way to better differentiate between maneuvers with share some properties.\n",
    "- The categorical variable RoadType could also be included in the model. While in this project it was manually labeled, automated labeling for new trips would be easy to implement by using the GPS and public road data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3-8-tensorflow04-telematic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
